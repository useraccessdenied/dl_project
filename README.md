# B-FQG: Bloom's Taxonomy-based Follow-up Question Generation

Implementation of the ACL 2025 Industry Track paper: **"From Recall to Creation: Generating Follow-Up Questions Using Bloom's Taxonomy and Grice's Maxims"**

This project implements a framework for generating cognitively scaffolded follow-up questions using Bloom's Taxonomy and evaluating them with Gricean-inspired metrics.

## Overview

The system uses Large Language Models to generate follow-up questions that progressively increase in cognitive complexity from basic recall (Level 1) to advanced creation (Level 6) according to Bloom's Revised Taxonomy. Generated questions are evaluated using the GriceWise framework based on conversational principles.

### Key Components
- **B-FQG Generator**: Creates 5 follow-up questions per seed question across cognitive levels
- **GriceWise Evaluator**: Assesses questions using Gricean Maxims (Quality, Quantity, Relation, Manner)
- **Interactive REPL**: Command-line interface for experimentation
- **Batch Processing**: Automated evaluation of multiple question sets

## Installation

```bash
# Clone the repository and navigate to it
cd /path/to/dl_project

# Install dependencies using uv
uv sync

# Run setup to download models and resources
uv run python pre_run.py
```

## Usage

### Interactive REPL (Recommended)
```bash
uv run python repl.py
```

The REPL provides an interactive interface where you can:
- Enter seed questions to generate follow-ups
- View evaluation scores in real-time
- Use commands like `/help`, `/examples`, `/batch`
- Save results to JSON files

### Batch Processing
```bash
uv run python main.py
```

This demonstrates the complete pipeline with sample seed questions and generates a results summary.

### API Usage
```python
from bfqg_generator import BFGQuestionGenerator
from gricewise_evaluator import GriceWiseEvaluator

# Generate questions
generator = BFGQuestionGenerator()
followups = generator.generate_followups("How do I adjust the volume?")

# Evaluate questions
evaluator = GriceWiseEvaluator()
evaluation = evaluator.evaluate_question_set("How do I adjust the volume?", followups)

print(followups)  # {'level_2': '...', 'level_3': '...', ...}
print(evaluation)  # {'level_2': {'logical_consistency': 0.8, ...}, ...}
```

## Bloom's Taxonomy Levels

The system generates questions at 5 cognitive levels:

1. **Level 1: Remember** (Seed questions - not generated by system)
2. **Level 2: Understand** - Explain ideas and concepts
3. **Level 3: Apply** - Use information in new situations
4. **Level 4: Analyze** - Break down into components and relationships
5. **Level 5: Evaluate** - Make judgments based on criteria
6. **Level 6: Create** - Generate new solutions and perspectives

## GriceWise Evaluation Metrics

Each question is evaluated on four dimensions:

- **Logical Consistency (Quality)**: How well the question follows from conversation context (NLI-based)
- **Informativeness (Quantity)**: Amount of new information provided (entropy-based)
- **Relevance (Relation)**: Relatedness to the conversation topic (embedding similarity)
- **Clarity (Manner)**: Ease of understanding (heuristic-based)

## Dataset

The original paper uses the `harshvivek14/Blooms-Followup-Questions` dataset from Hugging Face. This implementation includes sample seed questions but can be extended with the full dataset.

## Models Used

- **Question Generation**: FLAN-T5-base (via transformers)
- **Logical Consistency**: RoBERTa-large-MNLI (via transformers)
- **Relevance**: MiniLM-L6-v2 sentence transformers
- **Informativeness**: GPT-2-small (via transformers)

## Examples

### Basic Usage
```
üîç Enter a seed question or command: How do I start the car?

üîß Processing: How do I start the car?
Generating and evaluating questions...
Results saved automatically.
```

### Commands
- `/help` - Show help
- `/examples` - List sample questions
- `/batch` - Enter multiple questions
- `/save` - Export last result to JSON

## Dependencies

- `torch` - PyTorch for model inference
- `transformers` - Hugging Face transformers
- `sentence-transformers` - Embedding models
- `nltk` - Text processing
- `datasets` - Dataset loading
- `tqdm` - Progress bars

## Limitations

- Uses smaller, open-source models instead of GPT-4 for evaluation (as in original paper)
- Clarity evaluation uses heuristics due to spaCy compatibility issues
- Memory requirements for multiple large models

## Citation

If you use this implementation, please cite the original paper:

```bibtex
@inproceedings{yadav2025recall,
  title={From Recall to Creation: Generating Follow-Up Questions Using Bloom's Taxonomy and Grice's Maxims},
  author={Archana Yadav and Harshvivek Kashid and Pushpak Bhattacharyya and Medchalimi Sruthi and B JayaPrakash and Chintalapalli Raja Kullayappa and Mandala Jagadeesh Reddy},
  booktitle={Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 6: Industry Track)},
  pages={1322--1338},
  year={2025}
}
```

## License

This implementation is for academic and research purposes following the original paper's intent.
